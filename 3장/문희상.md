# Part3. AI로 쉽고 빠르게 데이터 활용하기

## '물어보새'

### 배민이 마주한 문제
- 대부분의 구성원들이 데이터를 활용해 업무를 수행
- 다만, SQL 구문에 다양한 조건을 반영해 작성하는 데 어려움
- 데이터를 적잘히 추출했는지 신뢰도에 대한 고민

### 목표
- 구성원의 데이터 리터러시 상향 평준화
1. 체계화 - 비즈니스 용어
2. 효율화 - 빠른 검색
3. 접근성 - 슬랙 이용
4. 자동화 - 365일 언제나 이용

### 기반 기술
- RAG, 랭체인
- LLMOps

### Text-to-SQL
- GPT-4o 만으로는 퀄리티가 떨어진다
- 사내 도메인 및 정책 이해도 부족, 리트리버 성능 저하

1. 데이터 보강
- 고도화된 테이블 메타 데이터 생성 작업
- 기존보다 풍부한 DDL 생성
- 비즈니스 용어집 + few-shot SQL 데이터 구축
- 비정형 데이터 수집 파이프라인

2. 검색 알고리즘 개발
- 질문이 모호 -> 구체화 필요 -> 질문의 의도와 관련 있는 적절한 용어 추출
- 쿼리 작성에 필요한 정보 추출 단계
  - 테이블 및 칼람 메타 정보, 테이블 DDL, 퓨샷 SQL 활용
  - 유사도 높은 정보 및 특정 단어 포함된 정보 등 여러 검색 알고리즘 조합 필요

3. 프롬프트 엔지니어링
- 분석가 페르소나 부여
- ReAct 방법 활용 -> 추론 및 검색 과정 결합

4. 실험 및 평가 시스템
- 예일 스파이더, 알리바바 버드와 같은 Text-to-SQL 성능 평가 리더보드
- 도메인 특화 문제 해결을 위해 따로 구축

5. 데이터 디스커버리
- 질문 분류 -> 점점 상세한 카테고리로 여러번 분류
- 데이터와 관련 없는 질문 기존에는 머신러닝 기반 분류 모델 -> 프롬프트 엔지니어링으로 분류 가능
- 대화 유형별 대응 방법 -> 싱글 턴, 가이디드 싱글 턴, 멀티 턴
- DDL 축소
- Plan and Solve Prompting 적용

6. 쿼리문 문법 검증
- 문법 체크 및 개선 방안 제안
- 검증은 두 단계로 구성
  - 칼람명 보정 체인 -> 칼람명 오류 없는지 체크 후 보정
  - 문법 검증 및 최적화 체인
- 추후 메타 정보를 벡터 스토어에 저장하고 사용자 가이드를 추가할 계획

---

## Polars로 데이터 처리를 더 빠르고 가볍게

### Polars의 장점
- Rust로 구현
- 메모리 관리에 대한 오버헤드가 없으며
- 안전한 동시성과 병렬 처리가 가능
- 메모리 캐싱과 재사용성 또한 높다

1. I/O 기능
- 다양한 포맷의 파일 지원
2. Lazy API
- 즉시 연산 수행하지 않고
- Query Plan 수립 후 최적의 시점에 연산 실행하는 Lazy Evaluation 방식
3. Streaming API
- 메모리에 담기 너무 큰 데이터를 처리할 때 일정 단위로 데이터를 가져와서 처리하는 방식

---

## 느낀점
- 결국 생성형 AI 모델이 지속적으로 발전되면서 기존의 방식들을 프롬프트 엔지니어링으로 간편화해도 비슷한 성능을 낼 수 있는 수준이 되는 것 같다
- 도메인, 비즈니스를 명확하게 이해하고 프롬프트로 AI가 이해하기 쉽게 풀어나는 역량은 앞으로도 더욱 중요한 역량이 될 것이며 미리미리 시도해보아야겠다는 생각이 들었다
- 그리고 생성형 AI 모델을 사용한다면 간편함으로 인해 발생가능한 여러 엣지 케이스들을 떠올리고 대책을 만드는 작업도 매우 중요한 것 같다
- 결국 AI의 잘못이라도 모든 책임은 인간이 져야하기에 앞으로는 AI를 책임질 수 있는 사람들만 살아남지 않을까 싶다





